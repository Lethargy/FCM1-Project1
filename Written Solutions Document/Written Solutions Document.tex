\documentclass[12pt]{article}

% Import packages
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{commath}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{bbm}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\usepackage{bookmark}
\usepackage{titlesec}
\setcounter{secnumdepth}{4}
\usepackage{tcolorbox}
\tcbuselibrary{breakable}
\allowdisplaybreaks

% Theorems and Lemmas
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{axiom}{Axiom}
\newtheorem{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

% Macros
\newcommand{\pr}[1]{\mathbb{P}\del{#1}}

\newcommand{\E}[1]{\mathbb{E}\sbr{#1}}

\newcommand{\var}[1]{\mathrm{Var}\del{#1}}


\begin{document}
\begin{titlepage}
\title{Written Solutions}
\author{Lu Vy}
\date{September 28, 2020}
\maketitle
\thispagestyle{empty}
\end{titlepage}

\section*{Problem 1.1}
\subsection*{Part A}
The existence of a positive root $\rho \in \del{0,\sqrt[n]{\beta}}$ is clear from the intermediate value theorem since $p\del{0} = -\beta < 0 < \beta^{\del{n-1}/n} = p\del{\sqrt[n]{\beta}}$. To show that this positive root is unique, it suffices to note that $p'\del{x} = nx^{n-1} + \del{n-1}x^{n-2}$ is positive whenever $x > 0$, so $p\del{x}$ is monotonically increasing on $\del{0,\infty}$.

\subsection*{Part B}
If $\rho$ is a root of $p\del{x}$, then we require
\begin{equation}
\label{1.1eq2}
\rho^n + \rho^{n-1} - \beta = 0.
\end{equation}
Implicit differentiation with respect to $\beta$ yields
$$
n\rho^{n-1} \od{\rho}{\beta} + \del{n-1}\rho^{n-2} \od{\rho}{\beta} - 1 = 0,
$$
from which we attain
\begin{equation}
\label{1.1eq1}
\od{\rho}{\beta} = \frac{1}{n\rho^{n-1}+\del{n-1}\rho^{n-2}}.
\end{equation}
Substitute this into $c\del{\beta,n}= \abs{\od{\rho}{\beta}}\abs{\frac{\beta}{\rho}} $:
\begin{align*}
c\del{\beta,n} &= \abs{\frac{\beta}{n\rho^{n}+\del{n-1}\rho^{n-1}}} \\
&= \abs{\frac{\beta}{n\rho^{n}+\del{n-1}\del{\beta - \rho^{n}}}} & \text{by \eqref{1.1eq2}} \\
&= \abs{\frac{\beta}{n\beta - \beta + \rho^n}} \\
&= \abs{\frac{1}{n-1 + \frac{1}{\beta}\rho^n}} = \frac{1}{n-1 + \frac{1}{\beta}\rho^n} & \rho > 0.
\end{align*}

\subsection*{Part C}
$$
0 < \rho < \sqrt[n]{\beta} \implies \frac{1}{n-1 + \frac{1}{\beta}} < c\del{\beta,n} < \frac{1}{n-1}
$$

\subsection*{Part D}
We have that $0 < c\del{\beta,n} < 1$ for every selection of $\beta > 0$ and $n \geq 2$, so $\rho$ is always well conditioned. Furthermore, if $n \to \infty$, then $c \to 0$. This means that $\rho$ is less sensitive to perturbations in $\beta$ for large $n$.

\section*{Problem 1.2}
\subsection*{Part A}
When $\beta = 2$, a ``1'' must lead the mantissa of any normalized floating point number. We may save a bit if we omit this ``1'' in the code-word representation (provided that the number represented is normalized). The floating point number is always assumed to be normalized, unless the exponent indicates otherwise. This is the case with the IEEE single and double precision standards.

\subsection*{Part B}
$$
1 + 5 + 3\del{12} = 42
$$
One bit is devoted to the sign. There are $16-\del{-15} + 1 = 32$ possible exponents, which may be represented by five bits. Finally, three bits are required for each of the twelve digits of the mantissa.

\subsection*{Part C}
If there are $7$ digits of accuracy to begin with, we may lose $5$ and still retain $2$.

\subsection*{Part D}
There is no risk of cancellation with $\mu$ (it is the sum of positive terms), so $\mu$ is less sensitive to perturbations. For a more rigorous argument, let us compare relative condition numbers (using the $L^1$ norm). The inner product $\mu = \sum_{i=1}^{n}\xi_i^2$ is a map from $\mathbb{R}^n \mapsto \mathbb{R}$, so
\begin{align*}
\kappa_\mu = 
\norm{\od{\mu}{\bf{x}}}_1\frac{\norm{\bf{x}}_1}{\mu}
=
\frac{1}{\abs{\sum_{i=1}^{n}\xi_i^2}}
\norm{
\begin{bmatrix}
2\xi_1 \\
\vdots \\
2\xi_n
\end{bmatrix}}_1
\norm{
\begin{bmatrix}
\xi_1 \\
\vdots \\
\xi_n
\end{bmatrix}
}_1
=
2\frac{\del{\sum_{i=1}^n\abs{\xi_i}}^2}{\sum_{i=1}^n\xi_i^2}.
\end{align*}
On the other hand, the inner product $\gamma = \sum_{i=1}^n\xi_i\eta_i$ is a map from $\mathbb{R}^n \times \mathbb{R}^n \mapsto \mathbb{R}$. We may treat this as a map from $\mathbb{R}^{2n} \mapsto \mathbb{R}$ with arguments $\del{\xi_1,\dots,\xi_n,\eta_1,\dots,\eta_n}$ so that we get
\begin{align*}
\kappa_\gamma =
\frac{1}{\abs{\sum_{i=1}^{n}\xi_i\eta_i}}
\norm{
\begin{bmatrix}
\eta_1 \\
\vdots \\
\eta_n \\
\xi_1 \\
\vdots \\
\xi_n
\end{bmatrix}}_1
\norm{
\begin{bmatrix}
\eta_1 \\
\vdots \\
\eta_n \\
\xi_1 \\
\vdots \\
\xi_n
\end{bmatrix}}_1
=
\abs{\frac{\del{\sum_{i=1}^n \abs{\eta_i} + \abs{\xi_i}}^2}{\sum_{i=1}^{n}\xi_i\eta_i}}.
\end{align*}
The difference is that the terms in the denominator of $\kappa_\gamma$ need not all be positive, so they may be chosen so that the denominator is near $0$.

\section*{Problem 1.3}
\subsection*{Part A}
For $n = 3$, it can be verified that the accumulation of error from this results in a sum of the form
\begin{align*}
\sigma = &\sum_{i=1}^{8} \xi_i\del{1 + \epsilon_{i,1}}\del{1 + \epsilon_{i,2}}\del{1 + \epsilon_{i,3}} & \abs{\epsilon_{i,j}} \leq u \\
&= \sum_{i=1}^{8} \xi_i\del{1 + \eta_i} & \abs{\eta_i} \lesssim 3u \\
&= \del{\xi_1 + \cdots + \xi_8} + \del{\xi_1\eta_1 + \cdots + \xi_8\eta_8}.
\end{align*}
The third form is most convenient for evaluating the absolute forward error, which is $\abs{\xi_1\eta_1 + \cdots + \xi_8\eta_8}$. Generalizing, the absolute forward error is
$$
\abs{\xi_1\eta_1 + \cdots + \xi_{2^k}\eta_{2^k}}, \quad \abs{\eta} \lesssim ku.
$$

\subsection*{Part B}
From the observation that
$$
\sigma = \sum_{i=1}^8 \xi_i \del{1 + \eta_i},
$$
we see that the computed solution actually solves the problem given by the data $$\del{\xi_1\del{1+\eta_1},\dots,\xi_8\del{1+\eta_8}}^T.$$
The absolute backward error is therefore
$$
\norm{
\begin{bmatrix}
\xi_1\del{1+\eta_1} \\
\vdots \\
\xi_8\del{1+\eta_8}
\end{bmatrix}
-
\begin{bmatrix}
\xi_1 \\
\vdots \\
\xi_8
\end{bmatrix}
}_1
=
\norm{
\begin{bmatrix}
\xi_1\eta_1 \\
\vdots \\
\xi_8\eta_8
\end{bmatrix}
}_1 = \sum_{i=1}^8 \abs{\xi_i\eta_i}.
$$
For general $n$, the absolute backward error by the $L_1$ norm is
$$
\sum_{i=1}^n \abs{\xi_i\eta_i}.
$$

\subsection*{Part C}
An upper bound for the absolute forward error is given by
\begin{align*}
\abs{\xi_1 \eta_1 + \cdots + \xi_{2^k}\eta_{2^k}} &\leq \abs{\xi_1} \abs{\eta_1} + \cdots + \abs{\xi_{2^k}} \abs{\eta_{2^k}} \\
&\lesssim ku\abs{\xi_1} + \cdots + ku\abs{\xi_{2^k}} \\
&= \del{\abs{\xi_1} + \cdots + \abs{\xi_{2^k}}}ku \\
&= \kappa_{\text{abs}}\log_2{n} u.
\end{align*}
Under the $L_1$ norm, the upper bound for the absolute backward error is the same:
\begin{align*}
\abs{\xi_1\eta_1} + \cdots + \abs{\xi_n \eta_n} &= \abs{\xi_1}\abs{\eta_1} + \cdots + \abs{\xi_n}\abs{\eta_n} \\
&\lesssim ku\abs{\xi_1} + \cdots + ku\abs{\xi_{2^k}} \\
&= \del{\abs{\xi_1} + \cdots + \abs{\xi_{2^k}}}ku \\
&= \kappa_{\text{abs}}\log_2{n} u.
\end{align*}
We have that $p\del{n} \approx \log_2 n$ for the binary fan-in tree, which grows slower than $p\del{n}\approx n$ of sequential summation. We have shown that the algorithm is not only weakly stable (because it can be bounded by $\kappa_{\text{abs}}p\del{n}u$ for some $p\del{n} \in \mathcal{O}\del{\log_2 n}$), but also backward stable because it solves a nearby problem (the backward error is small).
\end{document}